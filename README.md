# World-Models-Autonomous-Driving-Latest-Survey
A curated list of world model for autonmous driving. Keep updated.

## Announcement
Besides the wonderful papers we list below, we are very happy to announce that our group, NYU Learning Systems Laboratory, recently released a preprint titled: [AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data](https://arxiv.org/abs/2501.04969), the first joint-embedding predictive architecture (JEPA) based spatial world models for self-supervised representation learning of autonomous driving. Source code is available at [AD-L-JEPA-Release](https://github.com/HaoranZhuExplorer/AD-L-JEPA-Release). If this paper inspires you, you may consider cite it via:
```bibtex
@article{zhu2025ad,
  title={AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data},
  author={Zhu, Haoran and Dong, Zhenyuan and Topollai, Kristi and Choromanska, Anna},
  journal={arXiv preprint arXiv:2501.04969},
  year={2025}
}
```



## Papers
### 2025
#### ICML 2025
* DriveGPT: Scaling Autoregressive Behavior Models for Driving __`ICML 2025`__;  [Paper](https://arxiv.org/abs/2412.14415) [Demo](https://www.youtube.com/watch?v=-hLi44PfY8g)

#### CVPR 2025
* GEM: A Generalizable Ego-Vision Multimodal World Model for Fine-Grained Ego-Motion, Object Dynamics, and Scene Composition Control __`CVPR 2025`__; __`Generative AI`__; [Paper](https://arxiv.org/pdf/2412.11198), [Code to be released](https://github.com/vita-epfl/GEM)
* FUTURIST: Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers. __`CVPR 2025`__ [[Paper](https://arxiv.org/abs/2501.08303)] [[Code](https://github.com/Sta8is/FUTURIST)]
* DIO: Decomposable Implicit 4D Occupancy-Flow World Model  __`CVPR 2025`__ [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Diehl_DIO_Decomposable_Implicit_4D_Occupancy-Flow_World_Model_CVPR_2025_paper.pdf)

#### ICLR 2025
* LAW: Enhancing End-to-End Autonomous Driving with Latent World Model  __`ICLR 2025`__; __`End-to-End AD`__; [Paper](https://openreview.net/pdf?id=fd2u60ryG0), [Code](https://github.com/BraveGroup/LAW)
* PreWorld: Semi-Supervised Vision-Centric 3D Occupancy World Model for Autonomous Driving __`ICLR 2025`__; __`Occupancy Forecasting `__; __`Motion Planning `__; [Paper](https://openreview.net/pdf?id=rCX9l4OTCT), [Code](https://github.com/getterupper/PreWorld)
* AdaWM: Adaptive World Model based Planning for Autonomous Driving __`ICLR 2025`__; __`RL`__; __`Planning`__; [Paper](https://openreview.net/pdf?id=NEu8wgPctU)
* SSR: Navigation-Guided Sparse Scene Representation for End-to-End Autonomous Driving  __`ICLR 2025`__;  __`End-to-End AD`__; [Paper](https://openreview.net/pdf?id=Vv76fCYffN), [Code](https://github.com/PeidongLi/SSR)
* OccProphet: Pushing Efficiency Frontier of Camera-Only 4D Occupancy Forecasting with Observer-Forecaster-Refiner Framework __`ICLR 2025`__; __`Occupancy Forecasting `__; [Paper](https://openreview.net/pdf?id=vC7AlY1ytz), [Code to be released](https://github.com/JLChen-C/OccProphet)

#### AAAI 2025
* DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation __`AAAI 2025`__; __`Generative AI`__; __`LLM`__; [Paper](https://arxiv.org/pdf/2403.06845), [Website](https://drivedreamer2.github.io/), [Code](https://github.com/f1yfisher/DriveDreamer2)
* Drive-OccWorld: Driving in the Occupancy World: Vision-Centric 4D Occupancy Forecasting and Planning via World Models for Autonomous Driving __`AAAI 2025`__; __`Occupancy Forecasting `__; __`Planning `__; [Paper](https://arxiv.org/pdf/2408.14197), [Website](https://drive-occworld.github.io/), [Code](https://github.com/yuyang-cloud/Drive-OccWorld)

#### RSS 2025
* LOPR: Self-supervised Multi-future Occupancy Forecasting for Autonomous Driving [Paper](https://www.roboticsproceedings.org/rss21/p003.pdf)  __`RSS 2025`__;

#### Others
* FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving __`arxiv May`__; [Paper](https://arxiv.org/abs/2505.17685), [Code](https://github.com/MIV-XJTU/FSDrive)
* DriVerse: Navigation World Model for Driving Simulation via Multimodal Trajectory Prompting and Motion Alignment __`arxiv April`__; __`Generative AI`__; [Paper](https://arxiv.org/pdf/2504.18576), [Code](https://github.com/shalfun/DriVerse)
* Learning to Drive from a World Mode __`arxiv April`__; [Paper](https://www.arxiv.org/pdf/2504.19077)
* WoTE: End-to-End Driving with Online Trajectory Evaluation via BEV World Model __`arxiv April`__; [Paper](https://arxiv.org/abs/2504.01941), [Code](https://github.com/liyingyanUCAS/WoTE)
* AETHER: Geometric-Aware Unified World Modeling __`arxiv March`__; [Paper](https://arxiv.org/pdf/2503.18945), [Website](https://aether-world.github.io/)
* GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving __`Generative AI`__; [Paper](https://drive.google.com/file/d/1L_FwiQS0KvrzERaYeG08AA1GO5HpIMfq/view)
* Other Vehicle Trajectories Are Also Needed: A Driving World Model Unifies Ego-Other Vehicle Trajectories in Video Latent Space __`arxiv March`__; __`Generative AI`__; [Paper](https://arxiv.org/abs/2503.09215)
* $T^3$Former: Temporal Triplane Transformers as Occupancy World Models __`arxiv March`__; __`Occupancy Forecasting`__; [Paper](https://arxiv.org/pdf/2503.07338)
* InDRiVE: Intrinsic Disagreement-based Reinforcement for Vehicle Exploration through Curiosity-Driven Generalized World Model  __`arxiv March`__;  __`RL`__; [Paper](https://arxiv.org/abs/2503.05573)
* PIWM: Dream to Drive with Predictive Individual World Model __`TIV 2025`__; __`RL`__; [Paper](https://arxiv.org/abs/2501.16733), [Code](https://github.com/gaoyinfeng/PIWM)
* MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction  __`arxiv`__; __`Generative AI`__; [Paper](https://arxiv.org/abs/2502.11663) [Code](https://github.com/SenseTime-FVG/OpenDWM)
* Dream to Drive: Model-Based Vehicle Control Using Analytic World Models  __`arxiv`__;  __`Planning`__; [Paper](https://arxiv.org/abs/2502.10012)
* HERMES: A Unified Self-Driving World Model for Simultaneous 3D Scene Understanding and Generation  __`arxiv`__;  __`Generative AI`__;  __`LLM`__; [Paper](https://arxiv.org/pdf/2501.14729), [Code to be released](https://github.com/LMD0311/HERMES)
* AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data. __`arxiv`__; __`Pre-training`__; __`Self-supervised representation learning`__; [Paper](https://arxiv.org/abs/2501.04969), [Code](https://github.com/HaoranZhuExplorer/AD-L-JEPA-Release)
* Cosmos World Foundation Model Platform for Physical AI  __`arxiv`__; __`Foundation Model`__; [Paper](https://arxiv.org/abs/2501.03575), [Code](https://github.com/NVIDIA/Cosmos)

### 2024

#### NeurIPS 2024
*  DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model __`NeurIPS 2024`__; __`Dataset`__; [Paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/178f4666a84ecdd61e3b85145ed56484-Paper-Datasets_and_Benchmarks_Track.pdf), [Website](https://drivingdojo.github.io/), [Code](https://github.com/Robertwyq/Drivingdojo)
*  Vista: A Generalizable Driving World Model with High Fidelity and Versatile Controllability __`NeurIPS 2024`__; __`from Shanghai AI Lab`__;  __`Generative AI`__; [Paper](https://proceedings.neurips.cc/paper_files/paper/2024/file/a6a066fb44f2fe0d36cf740c873b8890-Paper-Conference.pdf), [Website](https://opendrivelab.com/Vista/), [Code](https://github.com/OpenDriveLab/Vista)

#### ECCV 2024
* DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving __`ECCV 2024`__; __`Generative AI`__; [Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06416.pdf), [Website](https://drivedreamer.github.io/), [Code](https://github.com/JeffWang987/DriveDreamer)
* Modelling Competitive Behaviors in Autonomous Driving Under Generative World Model __`ECCV 2024`__; __`RL`__; __`Trajectories Simulation`__; [Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/05085.pdf), [Code to be released](https://github.com/qiaoguanren/MARL-CCE)
* NeMo: Neural Volumetric World Models for Autonomous Driving __`ECCV 2024`__; __`End-to-End AD`__; __`Motion Planning `__; [Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02571.pdf)
* OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving __`ECCV 2024`__; __`Occupancy Forecasting`__; __`Motion Planning`__; [Paper](https://arxiv.org/pdf/2311.16038.pdf), [Code](https://github.com/wzzheng/OccWorld)
* Think2Drive: Efficient Reinforcement Learning by Thinking with Latent World Model for Autonomous Driving (in CARLA-v2)  __`ECCV 2024`__;  __`RL`__; [Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/06129.pdf), [Website](https://thinklab-sjtu.github.io/CornerCaseRepo/)
* FipTR: A Simple yet Effective Transformer Framework for Future Instance Prediction in Autonomous Driving __`ECCV 2024`__; __`Future Instance Prediction`__; [Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/11758.pdf), [Code](https://github.com/TabGuigui/FipTR)
* DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model __`ECCV 2024`__; __`Generative AI`__ [Paper](https://arxiv.org/pdf/2310.07771.pdf), [Code](https://github.com/shalfun/DrivingDiffusion)

#### CVPR 2024
* Drive-WM: Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving __`CVPR 2024`__; __`Generative AI`__; __`Planning`__; [Paper](https://arxiv.org/pdf/2311.17918.pdf), [Website](https://drive-wm.github.io/), [Code](https://github.com/BraveGroup/Drive-WM)
* DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving  __`CVPR 2024`__;  __`Pre-training`__; [Paper](https://arxiv.org/pdf/2405.04390)
* Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications  __`CVPR 2024`__;  __`Occupancy Forecasting `__; [Paper](https://arxiv.org/abs/2311.17663), [Code](https://github.com/haomo-ai/Cam4DOcc)
* GenAD: Generalized Predictive Model for Autonomous Driving __`CVPR 2024`__;  __`from Shanghai AI Lab`__ __`Generative AI`__; [Paper](https://arxiv.org/pdf/2403.09630.pdf), [Code](https://github.com/OpenDriveLab/DriveAGI) 
* ViDAR: Visual Point Cloud Forecasting enables Scalable Autonomous Driving  __`CVPR 2024`__; __`Pre-training`__;  __`from Shanghai AI Lab`__; __`NuScenes dataset`__ [Paper](https://arxiv.org/pdf/2312.17655), [Code](https://github.com/OpenDriveLab/ViDAR)
* UnO: Unsupervised Occupancy Fields for Perception and Forecasting __`CVPR 2024`__; __`Occupancy Forecasting`__; __`Pre-training`__; [Paper](https://arxiv.org/pdf/2406.08691) 

#### ICLR 2024
* Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion __`ICLR 2024`__; __`Future Point Cloud Prediction`__; __`from Waabi`__; [Paper](https://arxiv.org/abs/2311.01017)

#### ICRA 2024
* Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models  __`ICRA 2024`__; __`Planning`__ [Paper](https://arxiv.org/abs/2409.16663) 

#### Others
* InfinityDrive: Breaking Time Limits in Driving World Models __`arxiv 2024`__; __`Generative AI`__; [Paper](https://arxiv.org/abs/2412.01522v1), [Website](https://metadrivescape.github.io/papers_project/InfinityDrive/page.html)
* DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation __`arxiv 2024`__; __`Generative AI`__; __`4D Simulation`__; [Paper](https://arxiv.org/pdf/2410.13571), [Website](https://drivedreamer4d.github.io/), [Code](https://github.com/GigaAI-research/DriveDreamer4D)
* ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration __`arxiv 2024`__; __`Generative AI`__; __`4D Simulation`__; [Paper](https://arxiv.org/abs/2411.19548), [Website](https://recondreamer.github.io/), [Code](https://github.com/GigaAI-research/ReconDreamer)
* 2024-DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT. [Paper](https://arxiv.org/abs/2412.19505)  [Project Page](https://huxiaotaostasy.github.io/DrivingWorld/index.html) [Code](https://github.com/YvanYin/DrivingWorld)
* 2024-DOME: Taming Diffusion Model into High-Fidelity Controllable Occupancy World Model. [Paper](https://arxiv.org/abs/2410.10429)  [Project Page](https://gusongen.github.io/DOME)
* 2024-OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving [Paper](https://www.arxiv.org/abs/2409.03272)
* 2024-BEVWorld: A Multimodal World Model for Autonomous Driving via Unified BEV Latent Space  __`arxiv`__ [Paper](https://arxiv.org/abs/2407.05679)
* 2024-Planning with Adaptive World Models for Autonomous Driving  __`arxiv`__; __`Planning`__; [Paper](https://arxiv.org/pdf/2406.10714)
* 2024-OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving [Paper](https://arxiv.org/ab/2405.20337), [Code](https://github.com/wzzheng/OccSora)

### Before 2023
* 2023-ADriver-I: A General World Model for Autonomous Driving __`arxiv`__; __`Generative AI`__; __`NuScenes & one private dataset`__ [Paper](https://arxiv.org/pdf/2311.13549.pdf) 
* 2023-GAIA-1: A Generative World Model for Autonomous Driving __`arxiv`__; __`Generative AI`__; __`Wayve's private data`__ [Paper](https://arxiv.org/pdf/2309.17080.pdf)
* 2023-Neural World Models for Computer Vision __'PhD Thesis'__; __`from Wayve`__  [Paper](https://arxiv.org/pdf/2306.09179)
* 2022-Separating the World and Ego Models for Self-Driving __` ICLR 2022 workshop on Generalizable Policy Learning in the Physical World`__; __`from Yann Lecun's Group`__ [Paper](https://arxiv.org/abs/2204.07184), [Code](https://github.com/vladisai/pytorch-ppuu)
* 2022-SEM2: Enhance Sample Efficiency and Robustness of End-to-end Urban Autonomous Driving via Semantic Masked World Model  __`NeurIPS 2022 Deep Reinforcement Learning Workshop`__; __`RL`__; __`CARLA dataset`__ [Paper](https://arxiv.org/pdf/2210.04017.pdf)
* 2022-MILE: Model-Based Imitation Learning for Urban Driving __`NeurIPS 2022`__; __`RL`__; __`from Wayve`__ [Paper](https://arxiv.org/pdf/2210.07729.pdf), [Code](https://github.com/wayveai/mile)
* 2022-Iso-Dream: Isolating and Leveraging Noncontrollable Visual Dynamics in World Models __`NeurIPS 2022`__ [Paper](https://arxiv.org/pdf/2205.13817.pdf), [Code](https://github.com/panmt/iso-dream)
* 2021-FIERY: Future Instance Prediction in Bird's-Eye View from Surround Monocular Cameras __`ICCV 2019`__; __`Future Prediction`__; __`from Wayve`__; __`NuScenes, Lyft datasets`__ [Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_FIERY_Future_Instance_Prediction_in_Birds-Eye_View_From_Surround_Monocular_ICCV_2021_paper.pdf), [Code](https://github.com/wayveai/fiery)
* 2021-Learning to drive from a world on rails __`CVPR 2021 Oral`__; __`RL`__ [Paper](https://arxiv.org/pdf/2105.00636.pdf), [Project Page](https://dotchen.github.io/world_on_rails/), [Code](https://github.com/dotchen/WorldOnRails)
* 2019-Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic __`ICLR 2019`__; __`Future Prediction`__; __`from Yann Lecun's Group`__ [Paper](https://github.com/Atcold/pytorch-PPUU?tab=readme-ov-file), [Code](https://github.com/Atcold/pytorch-PPUU)
  
## Workshops/Challenges
* 2024-1X World Model Challenge  __`Challenges`__ [Link](https://github.com/1x-technologies/1xgpt)
* 2024-CVPR Workshop, Foundation Models for Autonomous Systems, Challenges, Track 4: Predictive World Model __`Challenges`__ [Link](https://opendrivelab.com/challenge2024/)

## Tutorials/Talks/
* 2023 __`from Wayve`__; [Video](https://www.youtube.com/watch?v=lNOs08byOhw)
* 2022-Neural World Models for Autonomous Driving [Video](https://www.youtube.com/watch?v=wMvYjiv6EpY)

## Surveys that Contain World Models for AD
* 2025-A Survey of World Models for Autonomous Driving __`arxiv`__ [Paper](https://arxiv.org/abs/2501.11260)
* 2024-World Models for Autonomous Driving: An Initial Survey __`arxiv`__ [Paper](https://arxiv.org/abs/2403.02622)
* 2024-Data-Centric Evolution in Autonomous Driving: A Comprehensive Survey of Big
Data System, Data Mining, and Closed-Loop Technologies __`arxiv`__ [Paper](https://arxiv.org/pdf/2401.12888.pdf)
* 2024-Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities __`arxiv`__ [Paper](https://arxiv.org/pdf/2401.08045.pdf)

## Other General World Model Papers
* 2025-What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models  __`ICML 2025`__ [Paper](https://www.arxiv.org/abs/2507.06952)
* 2025-Critiques of World Models [Paper](https://www.arxiv.org/abs/2507.05169)
* 2025-DREAMGEN: Unlocking Generalization in Robot Learning through Video World Models  __`from Nvidia`__  [Paper](https://arxiv.org/abs/2505.12705), [Code](https://github.com/NVIDIA/GR00T-Dreams)
* 2025-V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning __`from Meta`__  [Paper](https://ai.meta.com/research/publications/v-jepa-2-self-supervised-video-models-enable-understanding-prediction-and-planning/), [Code](https://github.com/facebookresearch/vjepa2)
* 2025-UniVLA: Learning to Act Anywhere with Task-centric Latent Actions __`arxiv 2025`__  [Paper](https://arxiv.org/abs/2505.06111), [Code](https://github.com/OpenDriveLab/UniVLA)
* 2025-Learning 3D Persistent Embodied World Models __`arxiv 2025`__ [Paper](https://www.arxiv.org/abs/2505.05495)
* 2025-AdaWorld: Learning Adaptable World Models with Latent Actions  __`ICML 2025`__ [Paper](https://arxiv.org/pdf/2503.18938)
* 2025-DreamerV3: Mastering diverse control tasks through world models __`Nature`__ [Paper](https://www.nature.com/articles/s41586-025-08744-2), [Code](https://github.com/danijar/dreamerv3)
* 2025-PhysTwin: Physics-Informed Reconstruction and Simulation of Deformable Objects from Videos [Paper](https://jianghanxiao.github.io/phystwin-web/phystwin.pdf), [Code](https://github.com/Jianghanxiao/PhysTwin)
* 2025-Intuitive physics understanding emerges from self-supervised pretraining on natural videos [Paper](https://arxiv.org/abs/2502.11831v1), [Code](https://github.com/facebookresearch/jepa-intuitive-physics)
* 2025-Do generative video models learn physical principles from watching videos? [Paper](https://arxiv.org/abs/2501.09038), [Code](https://github.com/google-deepmind/physics-IQ-benchmark), [Website](https://physics-iq.github.io/)
* 2024-PreLAR: World Model Pre-training with Learnable Action Representation __`ECCV 2024`__; __`Pretraining`__; __`RL`__; [Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03363.pdf), [Code](https://github.com/zhanglixuan0720/PreLAR)
* 2024-Understanding Physical Dynamics with Counterfactual World Modeling __`ECCV 2024`__; [Paper](https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03523.pdf), [Website](https://neuroailab.github.io/cwm-physics/), [Code](https://github.com/neuroailab/cwm_dynamics)
* 2024-Genie2: [Website](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/)
* 2024-WHALE: Towards Generalizable and Scalable World Models for Embodied Decision-making [Paper](https://arxiv.org/abs/2411.05619)
* 2024-How Far is Video Generation from World Model: A Physical Law Perspective [Paper](https://arxiv.org/abs/2411.02385)
* 2024-PIVOT-R: Primitive-Driven Waypoint-Aware World Model for Robotic Manipulation  __`NeurIPS 2024`__ [Paper](https://arxiv.org/abs/2410.10394) 
* 2024-RoboDreamer: Learning Compositional World Models for Robot Imagination [Paper](https://arxiv.org/abs/2404.12377)
* 2024-TD-MPC2: Scalable, Robust World Models for Continuous Control __`ICLR 2024`__ [Paper](https://openreview.net/pdf?id=Oxh5CstDJU)
* 2024-Hierarchical World Models as Visual Whole-Body Humanoid Controllers [Paper](https://arxiv.org/pdf/2405.18418)
* 2024-Efficient World Models with Time-Aware and Context-Augmented Tokenization __`ICML 2024`__ 
* 2024-3D-VLA: A 3D Vision-Language-Action Generative World Model __`ICML 2024`__ [Paper](https://arxiv.org/pdf/2403.09631.pdf)
* 2024-Newton from Archetype AI __`website`__ [Link](https://www.archetypeai.io/blog/introducing-archetype-ai---understand-the-real-world-in-real-time)
* 2024-MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators __`arxiv`__ [Paper](https://arxiv.org/pdf/2404.05014.pdf), [Code](https://github.com/PKU-YuanGroup/MagicTime)
* 2024-IWM: Learning and Leveraging World Models in Visual Representation Learning  __`arxiv`__, __`from Yann Lecun's Group`__ [Paper](https://arxiv.org/pdf/2403.00504.pdf)
* 2024-Video as the New Language for Real-World Decision Making __`arxiv`__, __`Deepmind`__ [Paper](https://arxiv.org/abs/2402.17139)
* 2024-Genie: Generative Interactive Environments __`Deepmind`__ [Paper](https://arxiv.org/abs/2402.15391v1), [Website](https://sites.google.com/view/genie-2024/home)
* 2024-Sora __`OpenAI`__, __`Generative AI`__ [Link](https://openai.com/sora), [Technical Report](https://openai.com/research/video-generation-models-as-world-simulators)
* 2024-LWM: World Model on Million-Length Video And Language With RingAttention __`arxiv`__; __`Generative AI`__ [Paper](https://arxiv.org/abs/2402.08268), [Code](https://github.com/LargeWorldModel/LWM)
* 2024-WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens __`arxiv`__; __`Generative AI`__ [Paper](https://arxiv.org/abs/2401.09985)
* 2024-Video prediction models as rewards for reinforcement learning __`NeurIPS 2024`__ [Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/d9042abf40782fbce28901c1c9c0e8d8-Paper-Conference.pdf), [Code](https://github.com/Alescontrela/viper_rl)
* 2024-V-JEPA: Revisiting Feature Prediction for Learning Visual Representations from Video __`from Yann Lecun's Group`__ [Paper](https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/), [Code](https://github.com/facebookresearch/jepa)
* 2023-STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning  __`NeurIPS 2023`__ [Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/5647763d4245b23e6a1cb0a8947b38c9-Paper-Conference.pdf), [Code](https://github.com/weipu-zhang/STORM)
* 2023-Facing Off World Model Backbones: RNNs, Transformers, and S4 __`NeurIPS 2023`__ [Paper](https://arxiv.org/abs/2307.02064)
* 2023-I-JEPA: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture __`CVPR 2023`__; __`from Yann Lecun's Group`__ [Paper](https://arxiv.org/abs/2301.08243), [Code](https://github.com/facebookresearch/ijepa)
* 2023-Temporally Consistent Transformers for Video Generation __`ICML 2023`__ [Paper](https://arxiv.org/abs/2210.02396), [Code](https://github.com/wilson1yan/teco)
* 2023-Learning to Model the World with Language __`arxiv`__ [Paper](https://arxiv.org/abs/2308.01399), [Code](https://github.com/jlin816/dynalang)
* 2023-Transformers are sample-efficient world models __`ICLR 2023`__;__`RL`__ [Paper](https://arxiv.org/pdf/2209.00588.pdf), [Code](https://github.com/eloialonso/iris)
* 2023-Gradient-based Planning with World Models __`arxiv`__; __`from Yann Lecun's Group`__; __`Planning`__; [Paper](https://arxiv.org/pdf/2312.17227)
* 2023-World Models via Policy-Guided Trajectory Diffusion __`arxiv`__; __`RL`__; [Paper](https://arxiv.org/pdf/2312.08533.pdf)
* 2023-DreamerV3: Mastering diverse domains through world models __`arxiv`__;__`RL`__; [Paper](https://arxiv.org/abs/2301.04104), [Code](https://github.com/danijar/dreamerv3)
* 2022-Daydreamer: World models for physical robot learning __`CoRL 2022`__; __`Robotics`__ [Paper](https://arxiv.org/abs/2206.14176), [Code](https://github.com/danijar/daydreamer)
* 2022-Masked World Models for Visual Control __`CoRL 2022`__; __`Robotics`__ [Paper](https://proceedings.mlr.press/v205/seo23a.html), [Code](https://github.com/younggyoseo/MWM) 
* 2022-A Path Towards Autonomous Machine Intelligence __`openreview`__; __`from Yann Lecun's Group`__; __`General Roadmap for World Models`__; [Paper](https://openreview.net/forum?id=BZ5a1r-kVsf); [Slides1](https://leshouches2022.github.io/SLIDES/compressed-yann-1.pdf), [Slides2](https://leshouches2022.github.io/SLIDES/lecun-20220720-leshouches-02.pdf), [Slides3](https://leshouches2022.github.io/SLIDES/lecun-20220720-leshouches-03.pdf); [Videos](https://www.youtube.com/playlist?list=PLEIq5bchE3R3Yl5taXdYA04a9kH9yvyGm)
* 2021-LEXA:Discovering and Achieving Goals via World Models __`NeurIPS 2021`__; [Paper](https://proceedings.neurips.cc/paper_files/paper/2021/hash/cc4af25fa9d2d5c953496579b75f6f6c-Abstract.html), [Website & Code](https://orybkin.github.io/lexa/)
* 2021-DreamerV2: Mastering Atari with Discrete World Models __`ICLR 2021`__; __`RL`__; __`from Google & Deepmind`__ [Paper](https://arxiv.org/pdf/2010.02193.pdf), [Code](https://github.com/danijar/dreamerv2)
* 2020-Dreamer: Dream to Control: Learning Behaviors by Latent Imagination __`ICLR 2020`__ [Paper](https://arxiv.org/abs/1912.01603), [Code](https://github.com/google-research/dreamer)
* 2019-Learning Latent Dynamics for Planning from Pixels __`ICML 2019`__ [Paper](https://proceedings.mlr.press/v97/hafner19a/hafner19a.pdf), [Code](https://github.com/google-research/planet)
* 2018-Model-Based Planning with Discrete and Continuous Actions __`arxiv`__; __`RL, Planning`__; __`from Yann Lecun's Group`__;  [Paper](https://arxiv.org/pdf/1705.07177)
* 2018-Recurrent world models facilitate policy evolution __`NeurIPS 2018`__; [Paper](https://papers.nips.cc/paper_files/paper/2018/file/2de5d16682c3c35007e4e92982f1a2ba-Paper.pdf), [Code](https://github.com/hardmaru/WorldModelsExperiments)

## Other Related Papers
* 2023-Occupancy Prediction-Guided Neural Planner for Autonomous Driving __`ITSC 2023`__; __`Planning, Neural Predicted-Guided Planning`__; __`Waymo Open Motion dataset`__ [Paper](https://arxiv.org/abs/2305.03303)

## Other Related Repos
[Awesome-World-Model](https://github.com/LMD0311/Awesome-World-Model),
[Awesome-World-Models-for-AD ](https://github.com/zhanghm1995/awesome-world-models-for-AD?tab=readme-ov-file#Table-of-Content),
[World models paper list from Shanghai AI lab](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving/blob/main/papers.md#world-model--model-based-rl),
[Awesome-Papers-World-Models-Autonomous-Driving](https://github.com/chaytonmin/Awesome-Papers-World-Models-Autonomous-Driving).
    
